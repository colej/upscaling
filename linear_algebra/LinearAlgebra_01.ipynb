{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a462dce",
   "metadata": {},
   "source": [
    "The point of this notebook is to help me refresh my knowledge of linear algebra and provide concrete examples of how to code linear algebra solutions both directly and with tools like scipy.\n",
    "\n",
    "I'll also be trying to relate these examples to solutions using numerical optimisation techniques as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171f43b",
   "metadata": {},
   "source": [
    "To star, let us consider the case where we have data $(x_i, y_i)$ with i $\\exists$ [1,N], inclusive with N>2. In this case, $x_i$ are the independent variables and $y_i$ are the dependent variables.\n",
    "\n",
    "--------------------------------------------------------------\n",
    "\n",
    "In general, we might be interested in finding the relationship between these two variables and using that relationship to make predictions about future data points yet to be taken.\n",
    "\n",
    "To do this, we are interested in regression analysis -- that is, determining the relationship between these two variables by finding the trend, or regressor, that best represents the average behaviour of the data and minimizes the differences between any given data points an the trend. \n",
    "\n",
    "We'll be using an example where we think that the trend is roughly linear, so our regressor with be a line, i.e. $f(x) = \\hat{y} = m x + b$.\n",
    "\n",
    "As I said, we'll be using linear algebra to carry out the regression analysis first! \n",
    "\n",
    "So, let's start out by defining our matrices.\n",
    "\n",
    "First, we have our dependent variables \n",
    "                                 $\\mathbf{Y}=\\left[ \\begin{matrix} y_1 \\\\\n",
    "                                                 y_2, \\\\\n",
    "                                                 \\vdots \\\\\n",
    "                                                 y_N \n",
    "                                   \\end{matrix} \\right]$, \n",
    "                                   \n",
    "which is a column vector that contains our observed data $y_i$.\n",
    "\n",
    "Next, we have our independent variables \n",
    "                                  $\\mathbf{X} = \\left[ \\begin{matrix} 1 & x_1 \\\\\n",
    "                                                  1 & x_2 \\\\\n",
    "                                                  \\vdots & \\vdots \\\\\n",
    "                                                  1 & x_N \n",
    "                                   \\end{matrix} \\right]$\n",
    "                                                  \n",
    "We also consider the general case where our data have associated variances $\\sigma^2_i$, this gives us the co-variance matrix\n",
    "$\\mathbf{C} = \\left[ \\begin{matrix} \\sigma^2_{y1} & 0 & \\cdots & 0 \\\\\n",
    "                                    0 & \\sigma^2_{y2} & \\cdots & 0 \\\\\n",
    "                                    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                                    0 & 0 & 0 & \\sigma^2_{yN}\n",
    "                                    \\end{matrix} \\right]$\n",
    "                                    \n",
    "                                    \n",
    "In matrix form, we can write our model as: ${\\bf Y}={\\bf X}\\beta$, where $\\beta$ contains the parameters that make our regressor best fit the data, i.e., $\\beta = \\left[ \\begin{matrix} m\\\\ b\\end{matrix}\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a4d6d",
   "metadata": {},
   "source": [
    "So, conceptually, what we want to do is identify the values of the parameters m and b  (i.e. $\\hat{\\beta}$) which best predict the relationship between our data $(x,y)$. \n",
    "\n",
    "To do this, we would minimize the square differences in the residuals between the data ${\\bf Y}$ and our model $mx+b$ (i.e. ${\\bf X}\\beta$). This motivates us to state the residual function $e_i=y_i-(mx_i+b)$ (i.e. $e={\\bf Y}-{\\bf X}\\beta$).\n",
    "\n",
    "Then our best fitting regressor is given by minimising the squared differences, given by $\\sum^N_i e_i^2 = \\sum^N_i \\left( y_i - (mx_i + b) \\right)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b0d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
